\documentclass[11pt]{article}

% Language setting
% Replace `english' with e.g. `spanish' to change the document language
\usepackage[english]{babel}

% Set page size and margins
% Replace `letterpaper' with`a4paper' for UK/EU standard size
\usepackage[a4paper,top=2cm,bottom=2cm,left=2cm,right=2cm,marginparwidth=1.5cm]{geometry}
\usepackage{indentfirst}
\setlength{\parindent}{20pt}
% Useful packages
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorlinks=true, allcolors=cyan]{hyperref}

\title{Mini Project 2 – Question Classification}
\author{Catarina Sousa 93695, Nelson Trindade 93743}
\date{November 10, 2021}

\begin{document}
\maketitle

\section{Models}
Our first model is a machine learning algorithm (Naïve Bayes). We start by parsing the input files to have labels and questions separated. We then pre-process the questions by \emph{tokenizing}, \emph{lowing} the words, \emph{stemming}, and \emph{lemmatizing}. At the end of our pre-process, we have a list with all the sentences. After processing, we use \emph{Count Vectorizer} to \emph{fit\_transform} our train set so that our model learns the mean and variance of the features of our set. We then transform our test set so that the model uses the same mean and variance that was calculated from the train set to transform the test set. We then use a \emph{Multinomial} Naïve Bayes distribution since it is suitable for the classification of discrete features.

Our second model is a SVM\footnote[1]{Support Vector Machine approach}, with TF-IDF\footnote[2]{Term Frequency–Inverse Document Frequency}. We parse the train and dev files just as we do in our first model, but then our pre-process is just a \emph{tokenizer}. We use the \emph{Count Vectorizer} to \emph{fit\_transform} our train set and transform our test set. After this, we use \emph{tfidfTransformer} (it has nothing to do with transformers, just the name) to \emph{fit\_transform} the training matrix and transform the testing matrix obtained by the \emph{Count Vectorizer}. These functions transform the count matrix into a normalized \emph{tf-idf} representation. We then use a Support Vector Machine to predict the test labels.

We started by doing small steps each time to be sure that what was done was correct. When we created our parser function, we tested it. When we created our processing function, we tested it. We did this technique to catch the errors as soon as possible. After having these functions functional, we started testing different approaches. 
In the Naïve Bayes approach, we tested different distributions until we concluded that the \emph{Multinomial} was the best. We tested with different pre-processing techniques until we reached the best one with this model (86.8\%).

In the Support Vector Machine approach, we also tested with different pre-processing techniques until we realized that using only the \emph{tokenizer} was the best approach. We first didn’t use the \emph{tfidfTransformer} functions and obtained an accuracy of 84\%. We started thinking about how we could improve our model and we decided that we wanted to use \emph{tf-idf}, since it reflects how important a word is in a corpus. We concluded that with \emph{tf-idf} we could get better results and that was what happened, we got 88.4\%.

The \emph{tokenizer}, the \emph{stemmer} and the \emph{lemmatizer} that we use are from the nltk library, while the \emph{MultinomialNB}, \emph{Count Vectorizer} and \emph{tfidfTransformer} are from the sklearn library.

To get the accuracy from our models, we compared the predicted labels with the correct labels using the \emph{sklearn.metrics.accuracy\_score()}.


\section{Results}
\begin{table}[!htbp]
    \centering
    \begin{tabular}{r|c|c|c|c|c|c}
    MODEL/LABEL & GEOGRAPHY & MUSIC & LITERATURE & HISTORY & SCIENCE & GENERAL \\\hline
    Naïve Bayes & 42/50 & 42/50 & 42/50 & 42/50 & 42/50 & 86.8\% \\
    SVM + TF-IDF & 42/50 & 42/50 & 42/50 & 42/50 & 42/50 & 88.4\%
    \end{tabular}
    \caption{\label{tab:widgets}Results obtained in the tests performed.}
\end{table}

\section{Error analysis}

In the first approach of SVM we had more pre-process, as we mentioned before. This led to a lower accuracy because some words were wrongly stemmed and lemmatized. For example, the stemmer would convert the word “upcoming” to the word “upcom”, which is incorrect. An example of a wrong lemmatization is the word “done”. If we don’t specify the part of speech, we get the word “done” instead of the word “do”.

We continued to analyze the errors and realized that our model didn’t consider the importance of a word, as we mentioned before. For example, our model failed to predict the label for the sentence “The Hubble Space Telescope was deployed by this space shuttle; 3 years later the Endeavour fixed its mirror | Discovery”. Our model predicted the label “History” instead of the label “Science”. This occurred because our model didn’t realize that the words “Hubble”, “space” and “telescope” were the most important ones to predict the correct label. Using the tfidfTransformer, our model started to predict it correctly.

By analyzing the errors, we realized that our model failed some sentences because some words in it appeared more than once in the predicted label (incorrect one). For example, in the sentence “The Babylonians kept abreast of the times using a form of this instrument seen here: Sundial” our model predicted the label History due to the word “Babylonians”. The word “Babylonian” appears in 6 sentences in the train set and 5 of them correspond to the label “History”. The answer to this sentence is very important to predict the right label, “Science”. The problem here is that the word “Sundial” never appears in the train set, so our model cannot predict that this sentence is related to “Science”.


\section{Future Work}

If we had more time to conclude our project, we would like to improve the pre-processing by implementing part-of-speech tagging, which would improve lemmatization’s results.
Although we aren’t allowed to use neural networks, if we had more time, we would like to test it to see which accuracy we would obtain.

\section{Bibliography}

\begin{itemize}
\item Fenix
\item and like this.
\end{itemize}

\end{document}